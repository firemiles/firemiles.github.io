<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>container - 标签 - Firemiles 的技术博客</title><link>https://blog.firemiles.top/tags/container/</link><description>container - 标签 - Firemiles 的技术博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miles.dev@outlook.comm (firemiles)</managingEditor><webMaster>miles.dev@outlook.comm (firemiles)</webMaster><lastBuildDate>Mon, 06 Aug 2018 22:59:59 +0000</lastBuildDate><atom:link href="https://blog.firemiles.top/tags/container/" rel="self" type="application/rss+xml"/><item><title>深入浅出容器网络（二）</title><link>https://blog.firemiles.top/2018/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%BA%8C/</link><pubDate>Mon, 06 Aug 2018 22:59:59 +0000</pubDate><author>firemiles</author><guid>https://blog.firemiles.top/2018/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%BA%8C/</guid><description><![CDATA[<div class="featured-image">
                <img src="https://firemiles-blog.oss-cn-shanghai.aliyuncs.com/2018-09-02-082400.jpg" referrerpolicy="no-referrer">
            </div><h1 id="容器网络组网类型">容器网络组网类型</h1>
<p>将容器介入容器网络前，要先搞清楚接入的容器网络到底使用了什么组网类型。常见的组网类型有<strong>underlay l2</strong>, <strong>overlay l2</strong> 和 <strong>overlay l3</strong>，还有一种是<strong>Host</strong>网络，也就是容器网络直接接入主机网络也就不存在所谓的容器网络，本文主要介绍前三种容器网络组网类型。</p>
<h2 id="underlay-网络">Underlay 网络</h2>
<p>underlay网络一般理解是底层网络，传统的网络组网就是underlay类型，区别于如今流行的隧道技术。</p>
<h3 id="underlay-l2">Underlay L2</h3>
<p>underlay l2网络就是2层（链路层）互通的底层网络，传统网络大多数属于这种类型。容器网络使用这种组网，常使用的技术就有IPVLAN L2和MACVLAN。</p>
<h4 id="macvlan">MACVLAN</h4>
<p>MACVLAN是linux提供的一种简单的网络子接口技术，它允许你在主机的一个网络接口上配置多个虚拟的网络接口，这些网络interface拥有自己独立的mac地址，也可以配置上独立的ip地址进行通信。macvlan下的容器和主机在同一网段中，共享同一广播域。macvlan和bridge比较类似，但是它省去了bridge的存在，网络效率更高，除此之外，macvlan也完美支持VLAN技术。</p>
<p>如果希望容器放在主机相同的网络中，享受已经存在的网络栈的各种优势，可以考虑macvlan。</p>
<ul>
<li><a href="https://docs.docker.com/network/macvlan/" target="_blank" rel="noopener noreffer">https://docs.docker.com/network/macvlan/</a>
</li>
<li><a href="http://cizixs.com/2017/02/14/network-virtualization-macvlan" target="_blank" rel="noopener noreffer">http://cizixs.com/2017/02/14/network-virtualization-macvlan</a>
</li>
</ul>
<h4 id="ipvlan">IPVLAN</h4>
<p>IPVLAN和MACVLAN类似，都是从一个主机接口虚拟出多个虚拟网络接口。一个重要区别是所有的虚拟接口都有相同的mac地址，而拥有不同的ip地址。因为所有的虚拟接口要共享mac地址，因此DNCP不能使用mac地址分配ip。</p>
<p>IPVLAN支持两种模式L2和L3，在Underlay L2中，我们使用L2模式。L2模式相比L3模式，父接口工作在类似交换机的模式，子接口可以收到并处理L2报文，相比L3有着更高的处理效率。</p>
<ul>
<li><a href="https://people.netfilter.org/pablo/netdev0.1/papers/IPVLAN-The-beginning.pdf" target="_blank" rel="noopener noreffer">https://people.netfilter.org/pablo/netdev0.1/papers/IPVLAN-The-beginning.pdf</a>
</li>
<li><a href="http://cizixs.com/2017/02/14/network-virtualization-macvlan" target="_blank" rel="noopener noreffer">http://cizixs.com/2017/02/14/network-virtualization-macvlan</a>
</li>
</ul>
<h3 id="underlay-l3">Underlay L3</h3>
<p>在Underlay L3组网中，可以选择使用IPVLAN的L3模式，该模式下ipvlan 有点像路由器的功能，它在各个虚拟网络和主机网络之间进行不同网络报文的路由转发工作。只要父接口相同，即使虚拟机/容器不在同一个网络，也可以互相 ping 通对方，因为 ipvlan 会在中间做报文的转发工作。</p>
<p>flannel 的 host-gw 组网，calico 的 BGP 组网方式都是 Underlay L3。</p>
<p></p>
<center> flannel网络架构（图片来自openshift）</center>
<h2 id="overlay-网络">Overlay 网络</h2>
<p>Overlay是在传统网络上虚拟出一个虚拟网络来，传统网络不再需要做任何适配，这样物理网络只对物理层的计算（物理机、虚拟化层管理网），虚拟网络只对应虚拟计算（虚拟机的业务IP）。</p>
<h3 id="overlay-l2">Overlay L2</h3>
<p></p>
<center> 基于 VXLAN 的 Overlay L2 组网 </center>
<p>传统的二层网络范围有限，Overlay l2网络是构建在传统网络上的L2网络，相较于传统l2网络，Overlay l2网络可以跨越多个数据中心，提供了大二层网络。构建在大二层网络上的VM或者容器在动态迁移具有很大的范围和很高的灵活性。</p>
<p>Overlay技术主要用VXLAN、NVGRE、GENEVE等，无论哪种协议都要求在发送方向对报文进行封装外层头，接收方向剥离外层头。VXLAN作为Overlay技术一个代表，是云计算的核心技术之一。</p>
<p>上图的容器Overlay L2网络就由VXLAN实现，通过在UDP包中封装L2报文，实现了容器跨主机进行L2通信。</p>
<ul>
<li><a href="https://feisky.gitbooks.io/sdn/basic/overlay.html" target="_blank" rel="noopener noreffer">https://feisky.gitbooks.io/sdn/basic/overlay.html</a>
</li>
<li><a href="http://techblog.d2-si.eu/2017/05/09/deep-dive-into-docker-overlay-networks-part-2.html" target="_blank" rel="noopener noreffer">http://techblog.d2-si.eu/2017/05/09/deep-dive-into-docker-overlay-networks-part-2.html</a>
</li>
</ul>
<h3 id="overlay-l3">Overlay L3</h3>
<p>Overlay L3组网类似Overlay L2，但是节点上会增加一个网关。每个节点上的容器都在同一个子网内，可以直接进行二层通信，但是跨节点的容器间通信只能走L3，都会经过网关转发，性能相比于Overlay L2较弱。但是牺牲的性能获得了更高的灵活性，跨节点的容器可以存在于不同的网段中。</p>
<p>flannel 的最新版本中，VXLAN 模式采用了 Overlay L3 模型。</p>
<p></p>
<center> flannel 基于 VXLAN 的 Overlay L3 组网 </center>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.sdnlab.com/21143.html" target="_blank" rel="noopener noreffer">https://www.sdnlab.com/21143.html</a>
</li>
</ul>]]></description></item><item><title>深入浅出容器网络（一）</title><link>https://blog.firemiles.top/2018/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B8%80/</link><pubDate>Sun, 27 May 2018 02:36:56 +0000</pubDate><author>firemiles</author><guid>https://blog.firemiles.top/2018/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B8%80/</guid><description><![CDATA[<div class="featured-image">
                <img src="https://firemiles-blog.oss-cn-shanghai.aliyuncs.com/2018-05-26-185616.jpg" referrerpolicy="no-referrer">
            </div><p>作为系列第一篇，先开个头，介绍模型和框架，后续再新增实战系列，理论是实战结合才能更好的理解容器网络的设计和实现原理。</p>
<h2 id="模型">模型</h2>
<p>容器技术已经火遍全球，业界迫切需要一个统一的容器网络模型，此时Docker提出了Container
Network Model（CNM），Kubernetes提出了Container Network Interface（CNI）。</p>
<h3 id="cnm">CNM</h3>
<p>Libnetwork是CNM的实现。它为Docker daemon和网络驱动程序之间提供了接口。网络控
制器负责将驱动和一个网络进行对接。每个驱动程序负责管理它所拥有的网络，以及为
该网络提供各种服务，例如IPAM负责ip分配。支持多个驱动程序多个网络同时共存。</p>
<p>网络驱动可以按提供方划分为原生驱动（libnetwork内置的或者Docker支持的）或者远
程驱动（第三方插件）。原生驱动包括none，bridge，overlay一粒macvlan。驱动也可
以按照适用范围划分为本地（单主机）的和全局的（多主机）。</p>
<h4 id="架构">架构</h4>
<p></p>
<ol>
<li>
<p>Network Sandbox</p>
<p>包含了一个容器的网络栈。包括了管理容器的网卡，路由表以及DNS设置。一种
Sandbox实现是通过linux的网络命名空间，一个FreeBSD Jail或者其他类似的概念。
一个Sandbox可以包含多个endpoints。</p>
</li>
<li>
<p>Endpoint</p>
<p>一个endpoint将Sandbox连接到network上。一个endpint的实现可以通过veth pair，
Open vSwitch internal port或者其他的方式。一个endpint只能属于一个network，
也只能属于一个sandbox。</p>
</li>
<li>
<p>Network</p>
<p>一个network是一组可以互相通信的endpoints组成。一个network的实现可以是linux
bridge，vlan或者其他方式。一个网络中可以包含多个endpoints。</p>
</li>
</ol>
<h4 id="接口">接口</h4>
<p></p>
<p>CNM的接口对比CNI模型较为复杂。提供了remote plugin方式，进行插件化开发。
remote plugin相较与CNI的命令行，更加友好一些，是通过http请求进行的。remote
plugin监听一个指定的端口，docker daemon直接通过这个端口和remote plugin进行
操作。</p>
<h5 id="调用过程">调用过程</h5>
<h6 id="create-network">Create Network</h6>
<p>这一系列调用发生再调用docker network create过程中。</p>
<ol>
<li>/IpamDriver.RequestPool: 创建subnetpool用于分配IP</li>
<li>/IpamDriver.RequestAddress: 为gateway获取IP</li>
<li>/NetworkDriver.CreateNetwork: 创建neutron network和subnet</li>
</ol>
<h6 id="create-container">Create Container</h6>
<p>这一系列调用发生再适用docker run，创建一个container过程中，也可以通过
<code>docker network connect</code> 触发。</p>
<ol>
<li>/IpamDriver.RequestAddress: 为容器获取IP</li>
<li>/NetworkDriver.CreateEndpiont: 创建neutron port</li>
<li>/NetworkDriver.Join: 为容器和port绑定</li>
<li>/NetworkDriver.ProgramExternalConnectivity:</li>
<li>/NetworkDriver.EndpointOperInfo</li>
</ol>
<h6 id="delete-network">Delete Network</h6>
<p>这一系列调用发生再使用 <code>docker network delete</code> 的过程中。</p>
<ol>
<li>/NetworkDriver.DeleteNetwork: 删除network</li>
<li>/IpamDriver.ReleaseAddress: 释放gateway的IP</li>
<li>/IpamDriver.ReleasePool: 删除subnetpool</li>
</ol>
<h3 id="cni">CNI</h3>
<p>CNI是由CoreOS提出的一种容器网络规范。已采纳该规范的包括Apache Mesos， Cloud
Foundry， Kubernetes，Kurma和rkt。另外Calico 和Weave这些项目也为CNI提供插件。</p>
<p></p>
<p>CNI的规范比较小巧。它规定了一个容器runtime和网络插件之间的简单的契约。这个契
约通过JSON的语法定义了CNI插件所需要提供的输入和输出。</p>
<p>一个容器可以被加入到不同插件所驱动的多个网络之中。一个网络有自己对应的插件和
唯一的名称。CNI插件需要提供两个命令：一个用来将网络接口加入到指定网络，另一
个用来将其移除。这两个接口分别再容器被创建和销毁时调用。</p>
<h4 id="cni工作流程">CNI工作流程</h4>
<p>docker 已经支持安装CNI插件。<a href="https://docs.docker.com/ee/ucp/kubernetes/install-cni-plugin/" target="_blank" rel="noopener noreffer">https://docs.docker.com/ee/ucp/kubernetes/install-cni-plugin/</a>
</p>
<p>容器runtime(e.g. kubernetes)首先需要分配一个网络命名空间以及一个容器ID。然后连同一些CNI配置
参数传给网络驱动。接着网络驱动会将容器间接入网络，并将分配的IP地址以JSON格
式返回给容器runtime。</p>
<p>目前，CNI的功能涵盖了IPAM, L2 和 L3。端口映射(L4)则由容器runtime自己负责。
CNI也没有规定端口映射的规则。</p>
<p>CNI支持与第三方IPAM的集成，可以用于任何容器runtime。CNM从设计上就仅仅支持
Docker。由于CNI简单的设计，许多人认为编写CNI插件会比编写CNM插件来得简单。</p>
<h3 id="cni和cnm的转化">CNI和CNM的转化</h3>
<p>CNI和CNM并非完全不可调和的两个模型。两者概念上有很多相似点，因此可以相互转化，
比如calico项目就支持两种接口模型。</p>
<p>CNI中的container与CNM的sandbox概念一致，CNI中的network与CNM中的network一致。
再CNI中，CNM中的endpoint被隐含在了ADD/DELETE操作中。CNI接口更加简洁，把更多
的工作托管给了容器的管理者和网络的管理者。从这个角度来说，CNI的ADD/DELETE接
口只实现了 <code>docker network connect</code> 和 <code>docker network disconnet</code> 两个命令。</p>
<p>kubernetes/contrib项目提供了一种从CNI向CNM转化的过程。其中原理很简单，就是直
接通过shell脚本执行 <code>docker network connect</code> 和 <code>docker network disconnect</code>
命令，来是心啊从CNI到CNM的转化。</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/containernetworking/cni" target="_blank" rel="noopener noreffer">https://github.com/containernetworking/cni</a>
</li>
<li><a href="https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/" target="_blank" rel="noopener noreffer">https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/</a>
</li>
<li><a href="http://dockone.io/article/1974" target="_blank" rel="noopener noreffer">http://dockone.io/article/1974</a>
</li>
<li><a href="http://www.cnblogs.com/xuxinkun/p/5707687.html" target="_blank" rel="noopener noreffer">http://www.cnblogs.com/xuxinkun/p/5707687.html</a>
</li>
</ul>
]]></description></item></channel></rss>